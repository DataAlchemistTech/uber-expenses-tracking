{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AWS-IAC-IAM-EC2-S3-Redshift.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHgHx3dRCJeu"
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import boto3\n",
        "import json\n",
        "import configparser\n",
        "from botocore.exceptions import ClientError\n",
        "import psycopg2\n",
        "import git"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTzlKykyKOZ_"
      },
      "source": [
        "def bucket_s3_exists(b):\n",
        "    s3 = boto3.resource('s3')\n",
        "    return s3.Bucket(b) in s3.buckets.all()\n",
        "\n",
        "def create_s3_bucket(b, folders):\n",
        "    client = boto3.client('s3')\n",
        "    client.create_bucket(Bucket=b, CreateBucketConfiguration={'LocationConstraint': 'us-east-2'})\n",
        "    if folders is not '':\n",
        "        fls = folders.split(',')\n",
        "        for f in fls:\n",
        "            client.put_object(Bucket= b, Body='', Key=f + '/')\n",
        "\n",
        "def upload_files_to_s3(file_name, b, folder, object_name, args=None):\n",
        "    \n",
        "    client = boto3.client('s3')\n",
        "\n",
        "    if object_name is None:\n",
        "        object_name = folder + \"/{fname}\".format(fname= os.path.basename(file_name)) \n",
        "\n",
        "    response = client.upload_file(file_name, b, object_name , ExtraArgs = args)\n",
        "\n",
        "    return response\n",
        "\n",
        "print('Downloading Github repository: Uber-expenses-tracking...')\n",
        "git.Git(\"C:/\").clone(\"git://github.com/Wittline/Uber-expenses-tracking.git\")\n",
        "print('Github repository downloaded')\n",
        "\n",
        "ACLargs = {'ACL':'authenticated-read' }\n",
        "bucket_names = {'uber-tracking-expenses-bucket-s3': 'unprocessed_receipts', 'airflow-runs-receipts':'eats,rides'}\n",
        "\n",
        "print('Creating the S3 buckets...')\n",
        "for k in bucket_names:\n",
        "    if not bucket_s3_exists(k):\n",
        "       create_s3_bucket(k, bucket_names[k])    \n",
        "\n",
        "print('S3 buckets created')\n",
        "\n",
        "print('Uploading the local receipts files to uber-tracking-expenses-bucket-s3 AWS S3 bucket...')\n",
        "files = glob.glob(\"localpath/receipts/*\")\n",
        "\n",
        "for file in files:\n",
        "    print(\"Uploading file:\", file)\n",
        "    print(upload_files_to_s3(file, 'uber-tracking-expenses-bucket-s3', 'unprocessed_receipts', None, ACLargs))\n",
        "\n",
        "\n",
        "print('Files uploaded to uber-tracking-expenses-bucket-s3 AWS S3 bucket')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2etmp72tsmj"
      },
      "source": [
        "# **Loading all the Params from the *dwh.cfg* file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzJam0bIjnqR"
      },
      "source": [
        "config = configparser.ConfigParser()\n",
        "config.read_file(open('localpath/dwh.cfg'))\n",
        "\n",
        "KEY                    = config.get('AWS','KEY')\n",
        "SECRET                 = config.get('AWS','SECRET')\n",
        "\n",
        "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
        "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
        "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
        "\n",
        "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
        "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
        "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
        "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
        "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
        "\n",
        "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
        "\n",
        "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
        "\n",
        "pd.DataFrame({\"Param\":\n",
        "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
        "              \"Value\":\n",
        "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
        "             })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pknfKjXuKFI"
      },
      "source": [
        "# **Creating clients for *IAM*, *EC2* and *Redshift* ressources**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30wjp0Cog-91"
      },
      "source": [
        "ec2 = boto3.resource('ec2',\n",
        "                       region_name=\"us-east-2\",\n",
        "                       aws_access_key_id=KEY,\n",
        "                       aws_secret_access_key=SECRET\n",
        "                    )\n",
        "\n",
        "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
        "                     aws_secret_access_key=SECRET,\n",
        "                     region_name='us-east-2'\n",
        "                  )\n",
        "\n",
        "redshift = boto3.client('redshift',\n",
        "                       region_name=\"us-east-2\",\n",
        "                       aws_access_key_id=KEY,\n",
        "                       aws_secret_access_key=SECRET\n",
        "                       )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgk0lDXxufZz"
      },
      "source": [
        "# **Creating the *IAM* Role that makes *Redshift* able to access S3 buckets (ReadOnly)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud-zHenhhz5f"
      },
      "source": [
        "try:\n",
        "    print(\"Creating new IAM Role\") \n",
        "    dwhRole = iam.create_role(\n",
        "        Path='/',\n",
        "        RoleName=DWH_IAM_ROLE_NAME,\n",
        "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
        "        AssumeRolePolicyDocument=json.dumps(\n",
        "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
        "               'Effect': 'Allow',\n",
        "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
        "             'Version': '2012-10-17'})\n",
        "    )    \n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    \n",
        "    \n",
        "print(\"Attaching Policy\")\n",
        "\n",
        "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
        "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
        "                      )['ResponseMetadata']['HTTPStatusCode']\n",
        "\n",
        "print(\"Get the IAM role ARN\")\n",
        "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
        "\n",
        "print(roleArn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l04q4zM4u_Ka"
      },
      "source": [
        "# **Creating *Redshift* Cluster**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25Q-zFUPiWw9"
      },
      "source": [
        "try:\n",
        "    response = redshift.create_cluster(        \n",
        "        \n",
        "        ClusterType=DWH_CLUSTER_TYPE,\n",
        "        NodeType=DWH_NODE_TYPE,\n",
        "        NumberOfNodes=int(DWH_NUM_NODES),\n",
        "\n",
        "\n",
        "        DBName=DWH_DB,\n",
        "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
        "        MasterUsername=DWH_DB_USER,\n",
        "        MasterUserPassword=DWH_DB_PASSWORD,\n",
        "        \n",
        "   \n",
        "        IamRoles=[roleArn]  \n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XHaeX-_vLtU"
      },
      "source": [
        "#**Redshift Cluster Details** (Run ths piece of code several times until status show **Available**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb9TaIeCiXb1"
      },
      "source": [
        "def prettyRedshiftProps(props):\n",
        "    pd.set_option('display.max_colwidth', -1)\n",
        "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
        "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
        "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
        "\n",
        "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
        "prettyRedshiftProps(myClusterProps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoidiGmLvqoX"
      },
      "source": [
        "# **Redshift Cluster endpoint and role ARN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMJRflWbigT9"
      },
      "source": [
        "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
        "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
        "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
        "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KjLCZWWv35g"
      },
      "source": [
        "# **Incoming TCP port to access to the cluster endpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz7hxmQKig5h"
      },
      "source": [
        "try:\n",
        "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
        "    defaultSg = list(vpc.security_groups.all())[0]\n",
        "    print(defaultSg)\n",
        "    defaultSg.authorize_ingress(\n",
        "        GroupName=defaultSg.group_name,\n",
        "        CidrIp='0.0.0.0/0',\n",
        "        IpProtocol='TCP',\n",
        "        FromPort=int(DWH_PORT),\n",
        "        ToPort=int(DWH_PORT)\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQg2iMXSwSZB"
      },
      "source": [
        "#**Checking the connection to the redshift cluster**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9L2Q-B3it7E"
      },
      "source": [
        "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
        "print(conn_string)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtT8fzm4syqo"
      },
      "source": [
        "print('Connecting to RedShift', conn_string)\n",
        "conn = psycopg2.connect(conn_string)\n",
        "print('Connected to Redshift')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2phmvGXVweoj"
      },
      "source": [
        "# **Cleaning and deleting all the resources** (Do not run the next lines until finish your experiments)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS3iSkoAi1s6"
      },
      "source": [
        "# #### CAREFUL!!\n",
        "# #-- Uncomment & run to delete the created resources\n",
        "# redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
        "# #### CAREFUL!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arbjtnbwi2L2"
      },
      "source": [
        "# myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
        "# prettyRedshiftProps(myClusterProps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq9RIguhi6SR"
      },
      "source": [
        "# iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
        "# iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}