# Building an ETL data pipeline with Apache Airflow, a data warehouse in AWS Redshift and a Power BI dashboard

<p align="justify">
  
Have you heard phrases like **Hungry? You're in the right place** or **Request a trip, hop in, and relax.** ? :roll_eyes: Both phrases are very common in our daily lives, they represent the emblems of the two most important businesses with <a href="https://qz.com/1889602/uber-q2-2020-earnings-eats-is-now-bigger-than-rides/"> millionaire revenues </a> from UBER. **Have you ever thought about how much money you spend on these services?** The goal of this project is to track the expenses of <a href="https://www.uber.com/">Uber Rides</a> and <a  href="https://www.ubereats.com/">Uber Eats</a> through a Data Engineering processes using technologies such as <a href="https://airflow.apache.org/">Apache Airflow</a>, <a href="https://aws.amazon.com/es/redshift/">AWS Redshift</a> and <a href="https://powerbi.microsoft.com/es-es/">Power BI</a>. Keep reading this article, I will show you a quick and easy way to automate everything step by step.

</p>

## What are the data sources?

<p align="justify">
 
Every time an Ubers Eat or Uber Rides service has ended, you will receive a payment receipt to your email, this receipt contains information about the details of the service, and is attached to the email with the extension **.eml**. Take a look at the image below, both receipts belong to the details sent by Uber about Eats and Rides services, this will be our original data sources, In my case, I downloaded all those receipts from my email to my local computer.

</p>



## Data modelling
## Infrastructure as Code in AWS
## Building an ETL data pipeline with Apache Airflow
## Visualizing AWS Redshift Data using Microsoft Power BI





